{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spring 2021 Livox lidar data processing\n",
    "\n",
    "Script to process raw .bin files from the livo lidar collected from March 2021 to June 2021\n",
    "S. Filhol 2021\n",
    "\n",
    "-------\n",
    "Part 0: Convert all point cloud to daily netcdf\n",
    "For each day:\n",
    "1. convert to LAS\n",
    "2. rotate LAS\n",
    "3. extract Geotiff\n",
    "4. compile daily geotiff to netcdf\n",
    "\n",
    "-------\n",
    "Part 1:  Find out when to convert bin file into DEMs\n",
    "\n",
    "1. get wind, temperature, flux, precip and snow depth from flux station\n",
    "2. identify events: precip, wind, transport.\n",
    "3. create list of timestamp to convert bin to netcdf. High temporal resolution during events, low temporal resolution in low activity\n",
    "\n",
    "-------\n",
    "Part 2\n",
    "\n",
    "1. write code to load files metadata to sort which file to process and post process\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wsn_client import query\n",
    "import xarray as xr\n",
    "import glob\n",
    "import pandas as pd\n",
    "import openpylivox as opl\n",
    "import pdal, json, os\n",
    "\n",
    "import configparser, logging, sys\n",
    "sys.path.append('/home/arcticsnow/github/SnowyOwl/')\n",
    "from appV2 import process_pcl as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert all point clouds to netcdf4 via geotiff\n",
    "\n",
    "dir_bin = '/media/arcticsnow/My Book/SO_spring_2021_processing/bin/'\n",
    "dir_netcdf = '/media/arcticsnow/My Book/SO_spring_2021_processing/netcdf/'\n",
    "dir_daily = '/media/arcticsnow/My Book/SO_spring_2021_processing/daily/'\n",
    "\n",
    "flist = glob.glob(dir_bin + '*.bin')\n",
    "flist.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame({'fname_archive':flist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of file metadata\n",
    "meta = pd.DataFrame({'fname_archive':flist})\n",
    "#extract timestamp from filename\n",
    "meta['tst'] = pd.to_datetime(meta.fname_archive.apply(lambda x: x.split('/')[-1][:-4]), format=\"%Y.%m.%dT%H-%M-%S\")\n",
    "meta['daily']=(meta.tst - pd.to_datetime('2021-03-11 00:00:00')).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 00:03:00')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.tst.diff().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to convert geotiff to netcdf\n",
    "import datetime\n",
    "from osgeo import gdal\n",
    "\n",
    "def fillnodata(fname, band=1, maxSearchDist=5, smoothingIterations=0):\n",
    "    ET = gdal.Open(fname, gdal.GA_Update)\n",
    "    ETband = ET.GetRasterBand(band)\n",
    "    result = gdal.FillNodata(targetBand=ETband, maskBand=None,\n",
    "                             maxSearchDist=maxSearchDist, smoothingIterations=smoothingIterations)\n",
    "    ETband = None\n",
    "    ET = None\n",
    "    \n",
    "\n",
    "def raster_to_ds_daily(dir_input, dir_output, compression=True, filename_format='%Y%m%d.nc'):\n",
    "    \"\"\"\n",
    "    function to store geotif into daily netcdf\n",
    "\n",
    "    :param dir_input: path to folder with geotif, str\n",
    "    :param dir_output: path to output folder, str\n",
    "    :param compression: copmress netcdf or not, defaults to True,  bool, optional\n",
    "    :param filename_format: output file name format following datetime system, defaults to '%Y%m%d.nc',  str, optional\n",
    "    \"\"\"    \n",
    "\n",
    "    # list filename\n",
    "    flist = glob.glob(dir_input + '*.tif')\n",
    "    flist.sort()\n",
    "    print(flist)\n",
    "    for f_rast in flist:\n",
    "        fillnodata(f_rast)\n",
    "        \n",
    "    # create dataframe of file metadata\n",
    "    meta = pd.DataFrame({'fname':flist})\n",
    "    #extract timestamp from filename\n",
    "    meta['tst'] = pd.to_datetime(meta.fname.apply(lambda x: x.split('/')[-1][:-4]))\n",
    "\n",
    "    # Create on netcdf file per day\n",
    "    for date in meta.tst.dt.day.unique():\n",
    "        # create time variable\n",
    "        time_var = xr.Variable('time', meta.tst.loc[meta.tst.dt.day==date])\n",
    "        # open raster files in datarray\n",
    "        geotiffs_da = xr.concat([xr.open_rasterio(i) for i in meta.fname.loc[meta.tst.dt.day==date]], dim=time_var)\n",
    "        # drop all NaN values\n",
    "        geotiffs_da = geotiffs_da.where(geotiffs_da!=-9999, drop=True)\n",
    "        # rename variables with raster band names\n",
    "        var_name = dict(zip(geotiffs_da.band.values, geotiffs_da.descriptions))\n",
    "        geotiffs_ds = geotiffs_da.to_dataset('band')\n",
    "        geotiffs_ds = geotiffs_ds.rename(var_name)\n",
    "\n",
    "        # save to netcdf file\n",
    "        fname_nc = dir_output + meta.tst.loc[meta.tst.dt.day==date].iloc[0].strftime(filename_format)\n",
    "        if compression:\n",
    "            encode = {\"min\":{\"compression\": \"gzip\", \"compression_opts\": 9}, \n",
    "                        \"max\":{\"compression\": \"gzip\", \"compression_opts\": 9},\n",
    "                        \"mean\":{\"compression\": \"gzip\", \"compression_opts\": 9},\n",
    "                        \"idw\":{\"compression\": \"gzip\", \"compression_opts\": 9},\n",
    "                        \"count\":{\"compression\": \"gzip\", \"compression_opts\": 9},\n",
    "                        \"stdev\":{\"compression\": \"gzip\", \"compression_opts\": 9}}\n",
    "            geotiffs_ds.to_netcdf(fname_nc,  encoding=encode, engine='h5netcdf')\n",
    "        else:\n",
    "            geotiffs_ds.to_netcdf(fname_nc)\n",
    "        print('File saved: ', fname_nc)\n",
    "\n",
    "        # clear memory cache before next loop\n",
    "        geotiffs_da = None\n",
    "        geotiffs_ds = None\n",
    "    for file in flist:\n",
    "        os.remove(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "Processing Day  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   :   0%|          | 0/465897 [00:00<?, ? pts/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- File moved\n",
      "\n",
      "CONVERTING OPL BINARY DATA, PLEASE WAIT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   : 100%|██████████| 465897/465897 [00:08<00:00, 57033.41 pts/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Point data was converted successfully to LAS, see file: /media/arcticsnow/My Book/SO_spring_2021_processing/daily/2021.04.01T23-57-00.bin.las\n",
      "     * OPL point data binary file has been deleted\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/arcticsnow/My Book/SO_spring_2021_processing/daily/2021.04.01T23-57-00.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-eb410af9cc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: cannot move '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-- File moved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_bin_to_las\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdir_daily\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-- Bin converted to las'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_rotate_point_clouds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'[-20:20]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'([-20, 10], [-5, 5])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/SnowyOwl/analysis/process.py\u001b[0m in \u001b[0;36mconvert_bin_to_las\u001b[0;34m(path_to_data)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mopl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertBin2LAS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeleteBin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' removed.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/arcticsnow/My Book/SO_spring_2021_processing/daily/2021.04.01T23-57-00.bin'"
     ]
    }
   ],
   "source": [
    "# Copy bin file for the day\n",
    "import shutil\n",
    "from appV2 import geotiff2netcdf as gn\n",
    "import process as pp\n",
    "sampling_interval=180\n",
    "\n",
    "\n",
    "for day in meta.daily.unique()[4:6]:\n",
    "    print('......')\n",
    "    print('Processing Day ', day)\n",
    "    daily = meta.loc[meta.daily==day]\n",
    "    for file in daily.fname_archive:\n",
    "        try:\n",
    "            tst_data = pd.to_datetime(file.split('/')[-1][:19],format=\"%Y.%m.%dT%H-%M-%S\")\n",
    "            if (tst_data.second + 60*tst_data.minute + 3600*tst_data.hour) % sampling_interval == 0:\n",
    "                shutil.copy(file, dir_daily)\n",
    "        except IOerror:\n",
    "            print('ERROR: cannot move ', file)\n",
    "    print('-- File moved')\n",
    "    pp.convert_bin_to_las(path_to_data=dir_daily)\n",
    "    print('-- Bin converted to las')\n",
    "    pp.tmp_rotate_point_clouds(z_range='[-20:20]', crop_corners='([-20, 10], [-5, 5])')\n",
    "    print('-- Rotated pcl')\n",
    "    pp.extract_dem(GSD= 0.02,\n",
    "                origin_x=-16.2,\n",
    "                origin_y=-4,\n",
    "                height=7.8,\n",
    "                width=22.3,\n",
    "                sampling_interval=180,\n",
    "                method='pdal',\n",
    "                path_to_data=dir_daily,\n",
    "                delete_las=True,\n",
    "                tif_to_zip=False)\n",
    "    print('-- Tif extracted')\n",
    "    raster_to_ds_daily(dir_daily, dir_netcdf)\n",
    "    print('-- daily Netcdf compiled')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25388, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create on netcdf file per day\n",
    "for day in meta.daily.unique():\n",
    "    # create time variable\n",
    "    time_var = xr.Variable('time', meta.tst.loc[meta.tst.dt.day==date])\n",
    "\n",
    "    # convert by daily batches. Use functions from process_pcl!!!!\n",
    "    pp.convert_bin_to_las()\n",
    "    pp.rotate_point_clouds()\n",
    "    pp.extract_dem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livox",
   "language": "python",
   "name": "livox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
